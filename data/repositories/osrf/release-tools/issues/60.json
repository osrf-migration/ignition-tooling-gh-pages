{"priority": "major", "kind": "bug", "repository": {"links": {"self": {"href": "data/repositories/osrf/release-tools.json"}, "html": {"href": "#!/osrf/release-tools"}, "avatar": {"href": "data/bytebucket.org/ravatar/{41811d3d-19d0-428e-a134-976595e72508}ts=python"}}, "type": "repository", "name": "release-tools", "full_name": "osrf/release-tools", "uuid": "{41811d3d-19d0-428e-a134-976595e72508}"}, "links": {"attachments": {"href": "data/repositories/osrf/release-tools/issues/60/attachments_page=1.json"}, "self": {"href": "data/repositories/osrf/release-tools/issues/60.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/release-tools/issues/60/watch"}, "comments": {"href": "data/repositories/osrf/release-tools/issues/60/comments_page=1.json"}, "html": {"href": "#!/osrf/release-tools/issues/60/repeat-failed-tests-to-identify-flaky-ones"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/release-tools/issues/60/vote"}}, "reporter": {"display_name": "Steve Peters", "uuid": "{2ccfed09-18b8-4921-8d58-15ef01092802}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D"}, "html": {"href": "https://bitbucket.org/%7B2ccfed09-18b8-4921-8d58-15ef01092802%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/1fb4816dad9e68337d3096f750951f6cd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsSP-1.png"}}, "nickname": "Steven Peters", "type": "user", "account_id": "557058:5de38267-b118-494c-aa76-4fab35448816"}, "title": "Repeat failed tests to identify flaky ones?", "component": null, "votes": 0, "watches": 1, "content": {"raw": "`ctest` has an option to repeat any failed tests (`--rerun-failed`). This could be used to identify flaky tests. It would take some work to manage the results files, since there would be duplicates of the repeated tests.\r\n\r\nPerhaps it could work as follows:\r\n\r\n1. Run `make test`, which will create junit files in the `test_results` folder.\r\n\r\n2. Copy the `test_results` folder to `test_results_original`.\r\n\r\n3. Run `make test ARGS=\"--rerun-failed\"`, which will run the failed tests once more and over-write the junit files in `test_results`.\r\n\r\n4. Repeat step 3 as desired.\r\n\r\nRunning this procedure should lead to a strictly decreasing number of test failures in `test_results`, since any tests that pass at least once will be shown as passing in the junit files in `test_results`. Any tests that failed in `test_results_original` but pass in `test_results` are flaky tests.\r\n\r\nThe Jenkins U/I would need a new element to display information about flaky tests, and we would also need some new logic to compare the results of `test_results_original` with `test_results`.", "markup": "markdown", "html": "<p><code>ctest</code> has an option to repeat any failed tests (<code>--rerun-failed</code>). This could be used to identify flaky tests. It would take some work to manage the results files, since there would be duplicates of the repeated tests.</p>\n<p>Perhaps it could work as follows:</p>\n<ol>\n<li>\n<p>Run <code>make test</code>, which will create junit files in the <code>test_results</code> folder.</p>\n</li>\n<li>\n<p>Copy the <code>test_results</code> folder to <code>test_results_original</code>.</p>\n</li>\n<li>\n<p>Run <code>make test ARGS=\"--rerun-failed\"</code>, which will run the failed tests once more and over-write the junit files in <code>test_results</code>.</p>\n</li>\n<li>\n<p>Repeat step 3 as desired.</p>\n</li>\n</ol>\n<p>Running this procedure should lead to a strictly decreasing number of test failures in <code>test_results</code>, since any tests that pass at least once will be shown as passing in the junit files in <code>test_results</code>. Any tests that failed in <code>test_results_original</code> but pass in <code>test_results</code> are flaky tests.</p>\n<p>The Jenkins U/I would need a new element to display information about flaky tests, and we would also need some new logic to compare the results of <code>test_results_original</code> with <code>test_results</code>.</p>", "type": "rendered"}, "assignee": null, "state": "new", "version": null, "edited_on": null, "created_on": "2016-02-22T20:54:21.988793+00:00", "milestone": null, "updated_on": "2016-03-31T19:21:33.640524+00:00", "type": "issue", "id": 60}